//! This is the Rust interface for the Tensorflow model `keras_model/q_learning_model1`.
//! That model is generated by the python script `keras_model/create_q_learning_model.py`

use std::path::Path;
use std::rc::Rc;

use tensorflow::{Graph, SavedModelBundle, SessionOptions, Tensor};

use crate::ql::model::{ACTION_SPACE, BATCH_SIZE};
use crate::ql::model::breakout_environment::{Action, Reward, State};
use crate::ql::model::model_function::{ModelFunction1, ModelFunction3};

const KERAS_MODEL_DIR: &str = "keras_model/q_learning_model_1";

pub struct QLearningTfModel1 {
    pub graph: Graph,
    pub bundle: SavedModelBundle,
    fn_predict_single: ModelFunction1,
    fn_batch_predict_future_reward: ModelFunction1,
    fn_train_model: ModelFunction3,
    fn_write_checkpoint: ModelFunction1,
    fn_read_checkpoint: ModelFunction1,
}

impl QLearningTfModel1 {
    pub fn init() -> Self {
        // we load the model as a graph from the path it was saved in
        let model_dir = Path::new(env!("CARGO_MANIFEST_DIR")).join(KERAS_MODEL_DIR);
        let mut graph = Graph::new();

        let bundle = SavedModelBundle::load(
            &SessionOptions::new(), &["serve"], &mut graph, model_dir,
        ).expect("Can't load model");

        // One way to get output names via saved_model_cli:
        // saved_model_cli show --dir /path/to/saved-model/ --all

        let fn_predict_single = ModelFunction1::new(&graph, &bundle, "predict_action", "state", "action");
        let fn_batch_predict_future_reward = ModelFunction1::new(&graph, &bundle, "batch_predict_future_reward", "state_batch", "reward_batch");
        let fn_train_model = ModelFunction3::new(&graph, &bundle, "train_model", "state_batch", "action_batch", "updated_q_values", "loss");
        let fn_write_checkpoint = ModelFunction1::new(&graph, &bundle, "write_checkpoint", "file", "file");
        let fn_read_checkpoint = ModelFunction1::new(&graph, &bundle, "read_checkpoint", "file", "dummy");

        QLearningTfModel1 {
            graph,
            bundle,
            fn_predict_single,
            fn_batch_predict_future_reward,
            fn_train_model,
            fn_write_checkpoint,
            fn_read_checkpoint,
        }
    }

    /// Predicts the next action based on the current state.
    ///
    /// # Arguments
    /// * `state` Game state Tensor [FRAME_SIZE_X, FRAME_SIZE_Y, WORLD_STATE_NUM_FRAMES]
    ///
    pub fn predict_action(
        &self,
        state: &Rc<State>,
    ) -> Action {
        let state = state.to_tensor();
        let r = self.fn_predict_single.apply(&self.bundle.session, &state);
        let r = r[0];
        assert!((0_i64..ACTION_SPACE as i64).contains(&r));
        r as Action
    }

    pub fn batch_predict_future_reward(
        &self,
        states: [&Rc<State>; BATCH_SIZE],
    ) -> [Reward; BATCH_SIZE] {
        let states = State::batch_to_tensor(&states);
        let r: Tensor<f32> = self.fn_batch_predict_future_reward.apply(&self.bundle.session, &states);
        assert_eq!(r.dims(), &[BATCH_SIZE as u64]);

        (*r).as_ref().try_into().unwrap()
    }

    /// Performs a single training step using a a batch of data.
    /// Returns the model's loss
    ///
    /// # Arguments
    /// * `state_batch` Tensor [BATCH_SIZE, FRAME_SIZE_X, FRAME_SIZE_Y, WORLD_STATE_NUM_FRAMES]
    /// * `action_batch` Tensor [BATCH_SIZE, 1]
    /// * `updated_q_values` Tensor [BATCH_SIZE, 1]
    ///
    pub fn train(
        &self,
        state_batch: [&Rc<State>; BATCH_SIZE],
        action_batch: [Action; BATCH_SIZE],
        updated_q_values: [f32; BATCH_SIZE],
    ) -> f32 {
        let state_batch_tensor = State::batch_to_tensor(&state_batch);

        let mut action_batch_tensor = Tensor::new(&[BATCH_SIZE as u64, 1]);
        for (i, action) in action_batch.into_iter().enumerate() {
            action_batch_tensor.set(&[i as u64, 0], action);
        }

        let mut updated_q_values_tensor = Tensor::new(&[BATCH_SIZE as u64, 1]);
        for (i, q) in updated_q_values.into_iter().enumerate() {
            updated_q_values_tensor.set(&[i as u64, 0], q);
        }
        let r = self.fn_train_model.apply::<_, _, _, f32>(&self.bundle.session, &state_batch_tensor, &action_batch_tensor, &updated_q_values_tensor);
        r[0]
    }

    pub fn write_checkpoint(
        &self,
        file: &str,
    ) -> String {
        let r = self.fn_write_checkpoint.apply::<String, String>(&self.bundle.session, &Tensor::from(file.to_string()));
        r[0].clone()
    }

    pub fn read_checkpoint(
        &self,
        file: &str,
    ) {
        self.fn_read_checkpoint.apply::<_, String>(&self.bundle.session, &Tensor::from(file.to_string()));
    }
}


#[cfg(test)]
mod test {
    use std::env::temp_dir;
    use std::rc::Rc;

    use rand::prelude::*;

    use crate::ql::frame_ring_buffer::FrameRingBuffer;
    use crate::ql::model::{ACTION_SPACE, BATCH_SIZE, FRAME_SIZE_X, FRAME_SIZE_Y};
    use crate::ql::model::breakout_environment::{Action, State};
    use crate::ql::model::q_learning_tf_model1::QLearningTfModel1;

    #[test]
    fn test_load_model() {
        QLearningTfModel1::init();
    }

    #[test]
    fn test_predict_single() {
        let model = QLearningTfModel1::init();
        let state = Rc::new(FrameRingBuffer::new(FRAME_SIZE_X, FRAME_SIZE_Y));
        let action = model.predict_action(&state);
        println!("action: {}", action)
    }

    #[test]
    fn test_batch_predict_future_reward() {
        let model = QLearningTfModel1::init();
        let states = (0..BATCH_SIZE).map(|_| Rc::new(FrameRingBuffer::new(FRAME_SIZE_X, FRAME_SIZE_Y))).collect::<Vec<_>>();
        let state_batch: [&Rc<State>; BATCH_SIZE] = states.iter().collect::<Vec<_>>().try_into().unwrap();
        let _future_rewards = model.batch_predict_future_reward(state_batch);
    }

    #[test]
    fn test_train() {
        let model = QLearningTfModel1::init();
        let states = (0..BATCH_SIZE).map(|_| Rc::new(FrameRingBuffer::random(FRAME_SIZE_X, FRAME_SIZE_Y))).collect::<Vec<_>>();
        let state_batch: [&Rc<State>; BATCH_SIZE] = states.iter().collect::<Vec<_>>().try_into().unwrap();
        let action_batch = [0; BATCH_SIZE].map(|_| thread_rng().gen_range(0..ACTION_SPACE) as Action);
        let updated_q_values = [0; BATCH_SIZE].map(|_| thread_rng().gen_range(0.0..3.0));
        model.train(state_batch, action_batch, updated_q_values);
    }

    #[test]
    fn test_save_and_load_model_ckpt() {
        let keras_model_checkpoint_dir = temp_dir().join("q_learning_model_1_ckpt");
        std::fs::create_dir(&keras_model_checkpoint_dir).expect("temp dir creation");
        let keras_model_checkpoint_file = keras_model_checkpoint_dir.join("checkpoint");
        let model = QLearningTfModel1::init();

        let path = model.write_checkpoint(keras_model_checkpoint_file.to_str().unwrap());
        log::debug!("saved model to '{}'", path);

        model.read_checkpoint(&keras_model_checkpoint_file.to_str().unwrap());
    }
}
